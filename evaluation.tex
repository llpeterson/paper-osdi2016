\section{Evaluation}

An SDNS system is considered effective if it imposes minimal data-plane
performance overhead, and offers an easy-to-understand but expressive way to
encode user-specific storage features. We present benchmarks to evaluate the
former, and sample applications to evaluate the latter.

\subsection{Data-plane Overhead}

\subsection{Code Overhead}

We created three applications with non-trivial storage features on top of
Syndicate. The first is a wide-area scientific data volume manager that spans
multiple administrative domains. The storage functional requirements are that
data is replicated to Amazon S3, and must be encrypted at rest and in transit.
Frequently request data must be cached in a private, shared CDN. The application
itself is concerned with translating user requests via a Web UI into requests to
the automount clients running on the user's VMs. The RG logic required to
implement this system is only ... lines of Python (to interface with S3) and the
UG logic is only XXX lines (to interface with the CDN and encrypt data).

The second application implements PGP email without requiring users to interact
with PGP keys. To users, the system looks like a webmail. The application is
local HTTP proxy that serves the web application, and uses a UG to store the
inbox and outgoing mail in the user's volume. It uses the legacy SMTP
network to announce message creation. This way, Alice's proxy will be
notified (via a signed SMTP message) that there is an encrypted message in
Bob's volume waiting for her, and will automatically fetch, authenticate,
decrypt, and serve it to her in her browser. The RG logic required to implement
this system is identical to that in the first application. The UG logic is only
XXX lines. It uses Blockstack to discover Alice's and Bob's keys,
encrypts the message to the recipients, and propagates the SMTP message to Bob
as when Alice writes to her outbox directory.

The third application is a scientific storage service for making existing iRODS
deployments available to remote Hadoop jobs, without requiring manual data
staging and transfer. This application is an extension of the first application,
but it additionally uses an AG to import iRODS data into a volume. The private
CDN automatically stages data close to the remote Hadoop nodes, and remote UGs
ensure that any communication failures between the AG and iRODS get resolved
before a read completes. The remote Hadoop jobs copy back their new data into a
separate iRODS home directory via a local RG. The UG code in this application is
the same as in the first, but the AG code is XXX lines of Python and the RG
code is XXX.

% [note by illyoung: The same UG logic as in the first is used but I/O interface
% is exposed via RESTful using Node.js. Also, an application that translates
% Hadoop filesystem operations to the Syndicate UG operations is implemented in
% Java.]

